{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# DBSCAN Clustering Pipeline for Health Phenotype Discovery\n",
    "\n",
    "This notebook implements a comprehensive DBSCAN clustering pipeline for identifying meaningful health subpopulations from the NHANES dataset.\n",
    "\n",
    "## Objectives\n",
    "- Perform systematic hyperparameter optimization using various metrics\n",
    "- Identify and interpret meaningful health subpopulations\n",
    "- Provide reproducible methodology for public health research\n",
    "- Achieve optimal clustering with Silhouette Score between 0.87-1.00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from itertools import product\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROJECT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROJECT CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define project root directory\n",
    "PROJECT_ROOT = os.path.abspath(os.path.dirname('__file__'))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'output_v2')\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, 'models')\n",
    "FIGURES_DIR = os.path.join(OUTPUT_DIR, 'figures')\n",
    "\n",
    "# Define subdirectories\n",
    "PHASE_DIRS = {\n",
    "    'data': os.path.join(DATA_DIR, 'raw'),\n",
    "    'processed': os.path.join(DATA_DIR, 'processed'),\n",
    "    'reports': os.path.join(OUTPUT_DIR, 'reports'),\n",
    "    'logs': os.path.join(OUTPUT_DIR, 'logs'),\n",
    "    'plots': os.path.join(FIGURES_DIR, 'plots')\n",
    "}\n",
    "\n",
    "MODEL_SUBDIRS = {\n",
    "    'baseline': os.path.join(MODELS_DIR, 'baseline'),\n",
    "    'tuned': os.path.join(MODELS_DIR, 'tuned'),\n",
    "    'final': os.path.join(MODELS_DIR, 'final')\n",
    "}\n",
    "\n",
    "OUTPUT_SUBDIRS = {\n",
    "    'metrics': os.path.join(OUTPUT_DIR, 'metrics'),\n",
    "    'predictions': os.path.join(OUTPUT_DIR, 'predictions'),\n",
    "    'cluster_profiles': os.path.join(OUTPUT_DIR, 'cluster_profiles')\n",
    "}\n",
    "\n",
    "# Create all directories\n",
    "all_dirs = [PROJECT_ROOT, DATA_DIR, OUTPUT_DIR, MODELS_DIR, FIGURES_DIR,\n",
    "            *PHASE_DIRS.values(), *MODEL_SUBDIRS.values(), *OUTPUT_SUBDIRS.values()]\n",
    "\n",
    "for dir_path in all_dirs:\n",
    "    if dir_path and not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(f\"\\n[INFO] Directory Structure:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data Directory: {DATA_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "def save_fig(figure, filename, subdir=None, formats=['png', 'pdf', 'svg']):\n",
    "    \"\"\"Save a matplotlib figure in multiple formats.\"\"\"\n",
    "    save_dir = FIGURES_DIR\n",
    "    if subdir:\n",
    "        save_dir = os.path.join(FIGURES_DIR, subdir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    saved_files = []\n",
    "    for fmt in formats:\n",
    "        filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "        figure.savefig(filepath, dpi=300, bbox_inches='tight', format=fmt)\n",
    "        saved_files.append(filepath)\n",
    "    return saved_files\n",
    "\n",
    "def save_model(model, filename, subdir=None):\n",
    "    \"\"\"Save a trained model using joblib.\"\"\"\n",
    "    save_dir = MODELS_DIR\n",
    "    if subdir and subdir in MODEL_SUBDIRS:\n",
    "        save_dir = MODEL_SUBDIRS[subdir]\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = os.path.join(save_dir, f\"{filename}.joblib\")\n",
    "    joblib.dump(model, filepath)\n",
    "    return filepath\n",
    "\n",
    "def save_data(data, filename, subdir=None, fmt='csv'):\n",
    "    \"\"\"Save data to file.\"\"\"\n",
    "    save_dir = OUTPUT_DIR\n",
    "    if subdir and subdir in OUTPUT_SUBDIRS:\n",
    "        save_dir = OUTPUT_SUBDIRS[subdir]\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = os.path.join(save_dir, f\"{filename}.{fmt}\")\n",
    "    if fmt == 'csv':\n",
    "        if hasattr(data, 'to_csv'):\n",
    "            data.to_csv(filepath, index=False)\n",
    "        else:\n",
    "            pd.DataFrame(data).to_csv(filepath, index=False)\n",
    "    return filepath\n",
    "\n",
    "print(\"\\n[OK] Configuration complete!\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading and Preprocessing\n",
    "\n",
    "Load the NHANES health dataset and prepare it for clustering analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NHANES health dataset\n",
    "data_path = os.path.join(PHASE_DIRS['data'], 'nhanes_health_data.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA LOADING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n[INFO] Dataset Shape: {df.shape}\")\n",
    "print(f\"[INFO] Number of samples: {df.shape[0]}\")\n",
    "print(f\"[INFO] Number of features: {df.shape[1]}\")\n",
    "print(f\"\\n[INFO] Column names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA EXPLORATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n[INFO] Data Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n[INFO] Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing Count'] > 0] if missing.sum() > 0 else \"  No missing values found!\")\n",
    "\n",
    "print(\"\\n[INFO] Numerical Statistics:\")\n",
    "print(df.describe().T.round(2))\n",
    "\n",
    "print(\"\\n[INFO] Categorical Variables:\")\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n  {col}:\")\n",
    "    print(df[col].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for clustering\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\n[STEP 1] Handling missing values...\")\n",
    "for col in df_processed.columns:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        if df_processed[col].dtype in ['float64', 'int64']:\n",
    "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "        else:\n",
    "            df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "print(\"  [OK] Missing values handled!\")\n",
    "\n",
    "# Encode categorical variables\n",
    "print(\"\\n[STEP 2] Encoding categorical variables...\")\n",
    "label_encoders = {}\n",
    "cat_columns = df_processed.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in cat_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"  [OK] Encoded {len(cat_columns)} categorical columns!\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\n[STEP 3] Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "feature_columns = df_processed.columns.tolist()\n",
    "X_scaled = scaler.fit_transform(df_processed)\n",
    "print(f\"  [OK] Scaled {len(feature_columns)} features!\")\n",
    "\n",
    "print(\"\\n[INFO] Preprocessed data shape:\", X_scaled.shape)\n",
    "\n",
    "# Save preprocessed data\n",
    "save_data(df_processed, 'preprocessed_data', 'processed')\n",
    "np.save(os.path.join(PHASE_DIRS['processed'], 'X_scaled.npy'), X_scaled)\n",
    "save_data(pd.DataFrame(feature_columns, columns=['feature']), 'feature_names')\n",
    "\n",
    "print(\"\\n[OK] Preprocessing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eps_determination",
   "metadata": {},
   "source": [
    "## Step 2: Optimal Epsilon Determination\n",
    "\n",
    "Determine the optimal epsilon value for DBSCAN using the k-distance graph and nearest neighbor analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimal_eps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal epsilon using k-distance graph\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMAL EPSILON DETERMINATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use k-nearest neighbors to find optimal epsilon\n",
    "k = 5  # typical choice for DBSCAN\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors_fit = neighbors.fit(X_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(X_scaled)\n",
    "\n",
    "# Get k-distance values (distance to k-th nearest neighbor)\n",
    "k_distances = np.sort(distances[:, k-1])\n",
    "\n",
    "# Plot k-distance graph\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# K-distance plot\n",
    "axes[0].plot(range(len(k_distances)), k_distances, 'b-', linewidth=0.5)\n",
    "axes[0].set_xlabel('Points (sorted by distance)', fontsize=12)\n",
    "axes[0].set_ylabel(f'{k}-Distance', fontsize=12)\n",
    "axes[0].set_title(f'K-Distance Graph (k={k})', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Find optimal epsilon using elbow method\n",
    "# Calculate second derivative to find elbow point\n",
    "diff1 = np.diff(k_distances)\n",
    "diff2 = np.diff(diff1)\n",
    "elbow_idx = np.argmax(diff2) + 1\n",
    "optimal_eps = k_distances[elbow_idx]\n",
    "\n",
    "axes[0].axhline(y=optimal_eps, color='r', linestyle='--', label=f'Optimal Epsilon = {optimal_eps:.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Histogram of k-distances\n",
    "axes[1].hist(k_distances, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=optimal_eps, color='r', linestyle='--', label=f'Optimal Epsilon = {optimal_eps:.3f}')\n",
    "axes[1].set_xlabel(f'{k}-Distance', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Distribution of K-Distances', fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(plt.gcf(), 'optimal_epsilon_analysis')\n",
    "\n",
    "print(f\"\\n[INFO] Optimal Epsilon Analysis:\")\n",
    "print(f\"  k value used: {k}\")\n",
    "print(f\"  Optimal epsilon: {optimal_eps:.4f}\")\n",
    "print(f\"  Min k-distance: {k_distances.min():.4f}\")\n",
    "print(f\"  Max k-distance: {k_distances.max():.4f}\")\n",
    "print(f\"  Mean k-distance: {k_distances.mean():.4f}\")\n",
    "print(f\"  Std k-distance: {k_distances.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyperparameter_optimization",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Optimization\n",
    "\n",
    "Perform systematic hyperparameter optimization for DBSCAN using various metrics including Silhouette Score, Calinski-Harabasz Index, and Davies-Bouldin Index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbscan_optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Systematic hyperparameter optimization\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "eps_values = np.arange(0.5, 3.0, 0.1)\n",
    "min_samples_values = [3, 5, 7, 10, 15]\n",
    "\n",
    "print(f\"\\n[INFO] Hyperparameter Search Space:\")\n",
    "print(f\"  Epsilon range: {eps_values.min():.1f} to {eps_values.max():.1f} (step: 0.1)\")\n",
    "print(f\"  Min samples values: {min_samples_values}\")\n",
    "print(f\"  Total combinations: {len(eps_values) * len(min_samples_values)}\")\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "print(\"\\n[INFO] Running grid search...\")\n",
    "for eps, min_samples in product(eps_values, min_samples_values):\n",
    "    # Fit DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    # Only calculate clustering metrics if we have valid clusters
    "    if n_clusters >= 2:\n",
    "        # Filter out noise points for metrics calculation\n",
    "        mask = labels != -1\n",
    "        if mask.sum() > n_clusters and len(set(labels[mask])) >= 2:\n",
    "            X_valid = X_scaled[mask]\n",
    "            labels_valid = labels[mask]\n",
    "            \n",
    "            silhouette = silhouette_score(X_valid, labels_valid)\n",
    "            calinski = calinski_harabasz_score(X_valid, labels_valid)\n",
    "            davies = davies_bouldin_score(X_valid, labels_valid)\n",
    "        else:\n",
    "            silhouette = -1\n",
    "            calinski = -1\n",
    "            davies = float('inf')\n",
    "    else:\n",
    "        silhouette = -1\n",
    "        calinski = -1\n",
    "        davies = float('inf')\n",
    "    \n",
    "    results.append({\n",
    "        'eps': eps,\n",
    "        'min_samples': min_samples,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'silhouette': silhouette,\n",
    "        'calinski': calinski,\n",
    "        'davies': davies,\n",
    "        'noise_ratio': n_noise / len(labels)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\n[OK] Grid search complete! {len(results)} combinations evaluated.\")\n",
    "\n",
    "# Save results\n",
    "save_data(results_df, 'hyperparameter_optimization_results', 'metrics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimization_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze optimization results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMIZATION RESULTS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Filter results with valid clusters\n",
    "valid_results = results_df[results_df['n_clusters'] >= 2].copy()\n",
    "\n",
    "print(f\"\\n[INFO] Valid configurations (n_clusters >= 2): {len(valid_results)}\")\n",
    "\n",
    "# Find best configurations\n",
    "best_silhouette = valid_results.loc[valid_results['silhouette'].idxmax()]\n",
    "best_calinski = valid_results.loc[valid_results['calinski'].idxmax()]\n",
    "best_davies = valid_results.loc[valid_results['davies'].idxmin()]\n",
    "\n",
    "print(f\"\\n[INFO] Best Configurations:\")\n",
    "print(f\"\\n  Best Silhouette Score:\")\n",
    "print(f\"    Epsilon: {best_silhouette['eps']:.2f}\")\n",
    "print(f\"    Min Samples: {int(best_silhouette['min_samples'])}\")\n",
    "print(f\"    Clusters: {int(best_silhouette['n_clusters'])}\")\n",
    "print(f\"    Silhouette: {best_silhouette['silhouette']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Best Calinski-Harabasz Index:\")\n",
    "print(f\"    Epsilon: {best_calinski['eps']:.2f}\")\n",
    "print(f\"    Min Samples: {int(best_calinski['min_samples'])}\")\n",
    "print(f\"    Clusters: {int(best_calinski['n_clusters'])}\")\n",
    "print(f\"    Calinski-Harabasz: {best_calinski['calinski']:.2f}\")\n",
    "\n",
    "print(f\"\\n  Best Davies-Bouldin Index (lower is better):\")\n",
    "print(f\"    Epsilon: {best_davies['eps']:.2f}\")\n",
    "print(f\"    Min Samples: {int(best_davies['min_samples'])}\")\n",
    "print(f\"    Clusters: {int(best_davies['n_clusters'])}\")\n",
    "print(f\"    Davies-Bouldin: {best_davies['davies']:.4f}\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Silhouette score heatmap\n",
    "pivot_silhouette = valid_results.pivot_table(\n",
    "    values='silhouette', index='min_samples', columns='eps', aggfunc='max'\n",
    ")\n",
    "sns.heatmap(pivot_silhouette, ax=axes[0, 0], cmap='viridis', annot=False)\n",
    "axes[0, 0].set_title('Silhouette Score Heatmap', fontsize=14)\n",
    "axes[0, 0].set_xlabel('Epsilon', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Min Samples', fontsize=12)\n",
    "\n",
    "# Calinski-Harabasz heatmap\n",
    "pivot_calinski = valid_results.pivot_table(\n",
    "    values='calinski', index='min_samples', columns='eps', aggfunc='max'\n",
    ")\n",
    "sns.heatmap(pivot_calinski, ax=axes[0, 1], cmap='plasma', annot=False)\n",
    "axes[0, 1].set_title('Calinski-Harabasz Index Heatmap', fontsize=14)\n",
    "axes[0, 1].set_xlabel('Epsilon', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Min Samples', fontsize=12)\n",
    "\n",
    "# Davies-Bouldin heatmap\n",
    "pivot_davies = valid_results.pivot_table(\n",
    "    values='davies', index='min_samples', columns='eps', aggfunc='min'\n",
    ")\n",
    "sns.heatmap(pivot_davies, ax=axes[1, 0], cmap='coolwarm_r', annot=False)\n",
    "axes[1, 0].set_title('Davies-Bouldin Index Heatmap (lower is better)', fontsize=14)\n",
    "axes[1, 0].set_xlabel('Epsilon', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Min Samples', fontsize=12)\n",
    "\n",
    "# Number of clusters\n",
    "pivot_clusters = valid_results.pivot_table(\n",
    "    values='n_clusters', index='min_samples', columns='eps', aggfunc='max'\n",
    ")\n",
    "sns.heatmap(pivot_clusters, ax=axes[1, 1], cmap='cubehelix', annot=True, fmt='.0f')\n",
    "axes[1, 1].set_title('Number of Clusters', fontsize=14)\n",
    "axes[1, 1].set_xlabel('Epsilon', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Min Samples', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(plt.gcf(), 'hyperparameter_optimization_heatmaps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select_optimal_params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimal parameters for target silhouette score 0.87-1.00\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMAL PARAMETER SELECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find configurations with silhouette score >= 0.87\n",
    "target_silhouette = 0.87\n",
    "high_quality_configs = valid_results[valid_results['silhouette'] >= target_silhouette].copy()\n",
    "\n",
    "print(f\"\\n[INFO] Configurations with Silhouette Score >= {target_silhouette}:\")\n",
    "print(f\"  Found {len(high_quality_configs)} configurations\")\n",
    "\n",
    "if len(high_quality_configs) > 0:\n",
    "    # Sort by silhouette score descending\n",
    "    high_quality_configs = high_quality_configs.sort_values('silhouette', ascending=False)\n",
    "    \n",
    "    print(\"\\n[INFO] Top 10 configurations:\")\n",
    "    print(high_quality_configs.head(10).to_string(index=False))\n",
    "    \n",
    "    # Select best configuration\n",
    "    optimal_config = high_quality_configs.iloc[0]\n",
    "else:\n",
    "    # If no configurations meet the target, use best available\n",
    "    print(\"\\n[WARNING] No configurations meet target silhouette score.\")\n",
    "    print(\"  Selecting best available configuration...\")\n",
    "    \n",
    "    # Find best configuration balancing all metrics\n",
    "    valid_results['combined_score'] = (\n",
    "        (valid_results['silhouette'] - valid_results['silhouette'].min()) / \n",
    "        (valid_results['silhouette'].max() - valid_results['silhouette'].min())\n",
    "    ) - (\n",
    "        (valid_results['davies'] - valid_results['davies'].min()) / \n",
    "        (valid_results['davies'].max() - valid_results['davies'].min())\n",
    "    )\n",
    "    \n",
    "    optimal_config = valid_results.loc[valid_results['combined_score'].idxmax()]\n",
    "\n",
    "OPTIMAL_EPS = optimal_config['eps']\n",
    "OPTIMAL_MIN_SAMPLES = int(optimal_config['min_samples'])\n",
    "\n",
    "print(f\"\\n[INFO] Selected Optimal Parameters:\")\n",
    "print(f\"  Epsilon: {OPTIMAL_EPS:.2f}\")\n",
    "print(f\"  Min Samples: {OPTIMAL_MIN_SAMPLES}\")\n",
    "print(f\"  Number of Clusters: {int(optimal_config['n_clusters'])}\")\n",
    "print(f\"  Silhouette Score: {optimal_config['silhouette']:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {optimal_config['calinski']:.2f}\")\n",
    "print(f\"  Davies-Bouldin Index: {optimal_config['davies']:.4f}\")\n",
    "print(f\"  Noise Ratio: {optimal_config['noise_ratio']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_clustering",
   "metadata": {},
   "source": [
    "## Step 4: Final DBSCAN Clustering\n",
    "\n",
    "Apply the optimal DBSCAN configuration to identify health subpopulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply_dbscan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply final DBSCAN clustering with optimal parameters\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL DBSCAN CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Fit final DBSCAN model\n",
    "final_dbscan = DBSCAN(eps=OPTIMAL_EPS, min_samples=OPTIMAL_MIN_SAMPLES)\n",
    "cluster_labels = final_dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Analyze clustering results\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"\\n[INFO] Clustering Results:\")\n",
    "print(f\"  Number of clusters: {n_clusters}\")\n",
    "print(f\"  Noise points: {n_noise} ({n_noise/len(cluster_labels):.2%})\")\n",
    "print(f\"  Clustered points: {len(cluster_labels) - n_noise} ({(len(cluster_labels) - n_noise)/len(cluster_labels):.2%})\")\n",
    "\n",
    "# Calculate final metrics (excluding noise)\n",
    "mask = cluster_labels != -1\n",
    "X_clustered = X_scaled[mask]\n",
    "labels_clustered = cluster_labels[mask]\n",
    "\n",
    "final_silhouette = silhouette_score(X_clustered, labels_clustered)\n",
    "final_calinski = calinski_harabasz_score(X_clustered, labels_clustered)\n",
    "final_davies = davies_bouldin_score(X_clustered, labels_clustered)\n",
    "\n",
    "print(f\"\\n[INFO] Final Clustering Metrics:\")\n",
    "print(f\"  Silhouette Score: {final_silhouette:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {final_calinski:.2f}\")\n",
    "print(f\"  Davies-Bouldin Index: {final_davies:.4f}\")\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "df_clustered = df.copy()\n",
    "df_clustered['Cluster'] = cluster_labels\n",
    "df_clustered['Is_Noise'] = cluster_labels == -1\n",
    "\n",
    "# Save clustered data\n",
    "save_data(df_clustered, 'clustered_data', 'predictions')\n",
    "\n",
    "# Save model\n",
    "save_model(final_dbscan, 'dbscan_final', 'final')\n",
    "\n",
    "print(\"\\n[OK] Clustering complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cluster_interpretation",
   "metadata": {},
   "source": [
    "## Step 5: Cluster Interpretation\n",
    "\n",
    "Analyze and interpret the identified health subpopulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cluster_profiles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster profile analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLUSTER PROFILE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate cluster statistics\n",
    "cluster_stats = []\n",
    "\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    \n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "    n_points = len(cluster_data)\n",
    "    \n",
    "    stats = {\n",
    "        'Cluster': cluster_id,\n",
    "        'Size': n_points,\n",
    "        'Percentage': n_points / len(df_clustered[~df_clustered['Is_Noise']]) * 100\n",
    "    }\n",
    "    \n",
    "    # Calculate mean for numerical columns\n",
    "    for col in df_clustered.select_dtypes(include=[np.number]).columns:\n",
    "        if col != 'Cluster':\n",
    "            stats[f'{col}_mean'] = cluster_data[col].mean()\n",
    "    \n",
    "    cluster_stats.append(stats)\n",
    "\n",
    "cluster_stats_df = pd.DataFrame(cluster_stats)\n",
    "\n",
    "print(f\"\\n[INFO] Cluster Sizes:\")\n",
    "for _, row in cluster_stats_df.iterrows():\n",
    "    print(f\"  Cluster {int(row['Cluster'])}: {int(row['Size'])} samples ({row['Percentage']:.1f}%)\")\n",
    "\n",
    "# Save cluster statistics\n",
    "save_data(cluster_stats_df, 'cluster_statistics', 'cluster_profiles')\n",
    "\n",
    "# Create detailed cluster profiles\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED CLUSTER PROFILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "key_features = ['Age', 'BMI', 'Systolic_BP', 'Total_Cholesterol', 'Blood_Glucose', \n",
    "                'HbA1c', 'HDL_Cholesterol', 'Diabetes', 'Hypertension']\n",
    "\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    \n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"CLUSTER {cluster_id}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Size: {len(cluster_data)} samples\")\n",
    "    \n",
    "    print(\"\\nKey Characteristics:\")\n",
    "    for feature in key_features:\n",
    "        if feature in cluster_data.columns:\n",
    "            if cluster_data[feature].dtype in ['float64', 'int64']:\n",
    "                mean_val = cluster_data[feature].mean()\n",
    "                print(f\"  {feature}: {mean_val:.2f}\")\n",
    "            else:\n",
    "                mode_val = cluster_data[feature].mode()[0]\n",
    "                pct = (cluster_data[feature] == mode_val).mean() * 100\n",
    "                print(f\"  {feature}: {mode_val} ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cluster_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive cluster visualization\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLUSTER VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Reduce dimensionality for visualization using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_clustered)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# 1. Cluster scatter plot\n",
    "scatter = axes[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_clustered, \n",
    "                             cmap='viridis', alpha=0.6, s=20)\n",
    "axes[0, 0].set_xlabel('First Principal Component', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Second Principal Component', fontsize=12)\n",
    "axes[0, 0].set_title('DBSCAN Clustering Results (PCA Visualization)', fontsize=14)\n",
    "plt.colorbar(scatter, ax=axes[0, 0], label='Cluster')\n",
    "\n",
    "# 2. Cluster size distribution\n",
    "cluster_sizes = df_clustered[~df_clustered['Is_Noise']].groupby('Cluster').size()\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(cluster_sizes)))\n",
    "axes[0, 1].bar(cluster_sizes.index, cluster_sizes.values, color=colors, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Cluster', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Number of Samples', fontsize=12)\n",
    "axes[0, 1].set_title('Cluster Size Distribution', fontsize=14)\n",
    "\n",
    "# 3. Key feature comparison across clusters\n",
    "feature_means = df_clustered[~df_clustered['Is_Noise']].groupby('Cluster')[['BMI', 'Systolic_BP', 'Age']].mean()\n",
    "feature_means_normalized = (feature_means - feature_means.min()) / (feature_means.max() - feature_means.min())\n",
    "\n",
    "x = np.arange(len(feature_means.columns))\n",
    "width = 0.8 / len(feature_means)\n",
    "\n",
    "for i, cluster_id in enumerate(feature_means.index):\n",
    "    axes[1, 0].bar(x + i * width, feature_means_normalized.loc[cluster_id], \n",
    "                   width, label=f'Cluster {cluster_id}')\n",
    "\n",
    "axes[1, 0].set_xlabel('Features', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Normalized Mean', fontsize=12)\n",
    "axes[1, 0].set_title('Key Features Across Clusters (Normalized)', fontsize=14)\n",
    "axes[1, 0].set_xticks(x + width * (len(feature_means) - 1) / 2)\n",
    "axes[1, 0].set_xticklabels(['BMI', 'Systolic BP', 'Age'])\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Silhouette score comparison\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "silhouette_vals = silhouette_samples(X_clustered, labels_clustered)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    cluster_silhouette_vals = silhouette_vals[labels_clustered == i]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    \n",
    "    size_cluster_i = cluster_silhouette_vals.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    axes[1, 1].fill_betweenx(\n",
    "        np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,\n",
    "        alpha=0.7\n",
    "    )\n",
    "    axes[1, 1].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "axes[1, 1].axvline(x=final_silhouette, color='red', linestyle='--', \n",
    "                   label=f'Average: {final_silhouette:.3f}')\n",
    "axes[1, 1].set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Cluster', fontsize=12)\n",
    "axes[1, 1].set_title('Silhouette Analysis by Cluster', fontsize=14)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(plt.gcf(), 'clustering_results_visualization')\n",
    "\n",
    "print(\"\\n[OK] Visualizations saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cluster_interpretation_summary",
   "metadata": {},
   "source": [
    "## Step 6: Health Phenotype Interpretation\n",
    "\n",
    "Interpret the identified clusters as meaningful health phenotypes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phenotype_interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health phenotype interpretation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HEALTH PHENOTYPE INTERPRETATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create phenotype descriptions based on cluster characteristics\n",
    "phenotype_descriptions = []\n",
    "\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    \n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "    \n",
    "    # Analyze key characteristics\n",
    "    avg_age = cluster_data['Age'].mean()\n",
    "    avg_bmi = cluster_data['BMI'].mean()\n",
    "    avg_sbp = cluster_data['Systolic_BP'].mean()\n",
    "    diabetes_pct = (cluster_data['Diabetes'] == 'Yes').mean() * 100\n",
    "    hypertension_pct = (cluster_data['Hypertension'] == 'Yes').mean() * 100\n",
    "    avg_hba1c = cluster_data['HbA1c'].mean()\n",
    "    \n",
    # Determine phenotype based on characteristics\n",
    "    phenotype_name = f\"Phenotype {cluster_id + 1}\"\n",
    "    description = []\n",
    "    \n",
    "    if avg_bmi >= 30:\n",
    "        description.append(\"High BMI (Obese)\")\n",
    "    elif avg_bmi >= 25:\n",
    "        description.append(\"Elevated BMI (Overweight)\")\n",
    "    else:\n",
    "        description.append(\"Normal BMI\")\n",
    "    \n",
    "    if avg_sbp >= 140:\n",
    "        description.append(\"High Blood Pressure\")\n",
    "    elif avg_sbp >= 120:\n",
    "        description.append(\"Elevated Blood Pressure\")\n",
    "    else:\n",
    "        description.append(\"Normal Blood Pressure\")\n",
    "    \n",
    "    if diabetes_pct >= 30:\n",
    "        description.append(\"High Diabetes Prevalence\")\n",
    "    elif diabetes_pct >= 10:\n",
    "        description.append(\"Moderate Diabetes Prevalence\")\n",
    "    \n",
    "    if avg_age >= 60:\n",
    "        description.append(\"Older Adults\")\n",
    "    elif avg_age >= 40:\n",
    "        description.append(\"Middle-aged\")\n",
    "    else:\n",
    "        description.append(\"Younger Adults\")\n",
    "    \n",
    "    phenotype_info = {\n",
    "        'Cluster': cluster_id,\n",
    "        'Phenotype_Name': phenotype_name,\n",
    "        'Sample_Size': len(cluster_data),\n",
    "        'Average_Age': round(avg_age, 1),\n",
    "        'Average_BMI': round(avg_bmi, 1),\n",
    "        'Average_Systolic_BP': round(avg_sbp, 1),\n",
    "        'Diabetes_Percentage': round(diabetes_pct, 1),\n",
    "        'Hypertension_Percentage': round(hypertension_pct, 1),\n",
    "        'Characteristics': \", \".join(description)\n",
    "    }\n",
    "    \n",
    "    phenotype_descriptions.append(phenotype_info)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER {cluster_id} - {phenotype_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Sample Size: {len(cluster_data)}\")\n",
    "    print(f\"  Average Age: {avg_age:.1f} years\")\n",
    "    print(f\"  Average BMI: {avg_bmi:.1f} kg/m²\")\n",
    "    print(f\"  Average Systolic BP: {avg_sbp:.1f} mmHg\")\n",
    "    print(f\"  Diabetes Prevalence: {diabetes_pct:.1f}%\")\n",
    "    print(f\"  Hypertension Prevalence: {hypertension_pct:.1f}%\")\n",
    "    print(f\"\\n  Characteristics: {', '.join(description)}\")\n",
    "\n",
    "# Save phenotype descriptions\n",
    "phenotype_df = pd.DataFrame(phenotype_descriptions)\n",
    "save_data(phenotype_df, 'phenotype_descriptions', 'cluster_profiles')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_summary",
   "metadata": {},
   "source": [
    "## Step 7: Summary and Model Export\n",
    "\n",
    "Generate comprehensive summary and export all results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = {\n",
    "    'Dataset': {\n",
    "        'Total_Samples': len(df),\n",
    "        'Total_Features': df.shape[1],\n",
    "        'Source': 'NHANES'\n",
    "    },\n",
    "    'Preprocessing': {\n",
    "        'Missing_Value_Handling': 'Median/Mode imputation',\n",
    "        'Encoding': 'Label encoding for categorical variables',\n",
    "        'Scaling': 'StandardScaler'\n",
    "    },\n",
    "    'DBSCAN_Parameters': {\n",
    "        'Epsilon': OPTIMAL_EPS,\n",
    "        'Min_Samples': OPTIMAL_MIN_SAMPLES,\n",
    "        'Metric': 'Euclidean'\n",
    "    },\n",
    "    'Clustering_Results': {\n",
    "        'Number_of_Clusters': n_clusters,\n",
    "        'Noise_Points': n_noise,\n",
    "        'Noise_Ratio': f\"{n_noise/len(cluster_labels):.2%}\"\n",
    "    },\n",
    "    'Quality_Metrics': {\n",
    "        'Silhouette_Score': f\"{final_silhouette:.4f}\",\n",
    "        'Calinski_Harabasz_Index': f\"{final_calinski:.2f}\",\n",
    "        'Davies_Bouldin_Index': f\"{final_davies:.4f}\"\n",
    "    },\n",
    "    'Optimization': {\n",
    "        'Method': 'Grid search over epsilon and min_samples',\n",
    "        'Target_Silhouette': '>= 0.87',\n",
    "        'Achieved_Silhouette': f\"{final_silhouette:.4f}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n[DATASET]\")\n",
    "print(f\"  Total Samples: {summary['Dataset']['Total_Samples']}\")\n",
    "print(f\"  Total Features: {summary['Dataset']['Total_Features']}\")\n",
    "print(f\"  Source: {summary['Dataset']['Source']}\")\n",
    "\n",
    "print(f\"\\n[DBSCAN PARAMETERS]\")\n",
    "print(f\"  Epsilon: {summary['DBSCAN_Parameters']['Epsilon']}\")\n",
    "print(f\"  Min Samples: {summary['DBSCAN_Parameters']['Min_Samples']}\")\n",
    "\n",
    "print(f\"\\n[CLUSTERING RESULTS]\")\n",
    "print(f\"  Number of Clusters: {summary['Clustering_Results']['Number_of_Clusters']}\")\n",
    "print(f\"  Noise Points: {summary['Clustering_Results']['Noise_Points']}\")\n",
    "\n",
    "print(f\"\\n[QUALITY METRICS]\")\n",
    "print(f\"  Silhouette Score: {summary['Quality_Metrics']['Silhouette_Score']}\")\n",
    "print(f\"  Calinski-Harabasz Index: {summary['Quality_Metrics']['Calinski_Harabasz_Index']}\")\n",
    "print(f\"  Davies-Bouldin Index: {summary['Quality_Metrics']['Davies_Bouldin_Index']}\")\n",
    "\n",
    "# Check if target achieved\n",
    "silhouette_value = float(summary['Quality_Metrics']['Silhouette_Score'])\n",
    "if silhouette_value >= 0.87:\n",
    "    print(f\"\\n[TARGET ACHIEVED] ✓\")\n",
    "    print(f\"  Silhouette Score {silhouette_value:.4f} >= 0.87\")\n",
    "else:\n",
    "    print(f\"\\n[TARGET NOT FULLY ACHIEVED]\")\n",
    "    print(f\"  Silhouette Score {silhouette_value:.4f} < 0.87\")\n",
    "    print(f\"  Consider further hyperparameter tuning or data preprocessing.\")\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame([summary])\n",
    "save_data(summary_df, 'final_summary', 'metrics')\n",
    "\n",
    "# Export cluster profiles\n",
    "cluster_profiles_export = df_clustered[~df_clustered['Is_Noise']].copy()\n",
    "cluster_profiles_export['Phenotype'] = cluster_profiles_export['Cluster'].map(\n",
    "    lambda x: phenotype_df[phenotype_df['Cluster'] == x]['Phenotype_Name'].values[0]\n",
    "    if len(phenotype_df[phenotype_df['Cluster'] == x]) > 0 else f\"Phenotype {x+1}\"\n",
    ")\n",
    "save_data(cluster_profiles_export, 'cluster_profiles', 'cluster_profiles')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nResults saved to:\")\n",
    "print(f\"  - {OUTPUT_DIR}/metrics/\")\n",
    "print(f\"  - {OUTPUT_DIR}/predictions/\")\n",
    "print(f\"  - {OUTPUT_DIR}/cluster_profiles/\")\n",
    "print(f\"  - {MODELS_DIR}/final/\")\n",
    "print(f\"  - {FIGURES_DIR}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_artifacts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all artifacts\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAVING ARTIFACTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save scaler and encoders\n",
    "joblib.dump(scaler, os.path.join(MODELS_DIR, 'scaler.joblib'))\n",
    "joblib.dump(label_encoders, os.path.join(MODELS_DIR, 'label_encoders.joblib'))\n",
    "\n",
    "# Save configuration\n",
    "config_dict = {\n",
    "    'optimal_eps': OPTIMAL_EPS,\n",
    "    'optimal_min_samples': OPTIMAL_MIN_SAMPLES,\n",
    "    'n_clusters': n_clusters,\n",
    "    'silhouette_score': final_silhouette,\n",
    "    'calinski_harabasz': final_calinski,\n",
    "    'davies_bouldin': final_davies,\n",
    "    'feature_columns': feature_columns\n",
    "}\n",
    "\n",
    "import json\n",
    "config_path = os.path.join(OUTPUT_DIR, 'dbscan_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\n[OK] Artifacts saved:\")\n",
    "print(f\"  - Scaler: {MODELS_DIR}/scaler.joblib\")\n",
    "print(f\"  - Label Encoders: {MODELS_DIR}/label_encoders.joblib\")\n",
    "print(f\"  - Final Model: {MODELS_DIR}/final/dbscan_final.joblib\")\n",
    "print(f\"  - Configuration: {config_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DBSCAN CLUSTERING PIPELINE - COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
